{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action-Object-Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the results from the [prior notebook](01_extract_individual_sentences.ipynb) where spaCy has been used to split up the extracted text into its individual sentences.\n",
    "\n",
    "It takes these individual sentences to do a pre-selection or filtering of sentences containing certain words. Words used for this pre-selection are for example 'meeting', 'review', 'send', 'call'. They imply an intent and are saved as 'targets'. \n",
    "\n",
    "Using spaCy again, from these target-sentences action-object-pairs (also called verb-object-pairs) will be extracted.\n",
    "Finally the DF is being exploded to create a better base for further experiments, conducted in the [next notebook](03_filter_action_object_pairs.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../data/processed/avocado_train_individual_sentences.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create list of words for filtering requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a list of words that imply certain intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_words = [\n",
    "    'dinner', 'lunch', 'breakfast', 'meeting', 'appointment', 'reminder', 'review', 'send me', \n",
    "    'need', 'how', 'schedule', 'please', 'send', 'sent', 'join', 'make sure', \n",
    "    'discuss', 'email', 'attend', 'call', 'provide', 'help', 'are there', 'are you', 'available', \n",
    "    'can i', 'can you', 'can we', 'can he', 'can she', 'can they', 'could you', 'could we', 'could i', \n",
    "    'did you', 'did i', 'did we', 'did he', 'did she', 'did they', 'do you', 'do they', 'does he', 'does she', 'do we',\n",
    "    'do not', \"don't\", 'want', 'does that', 'does this', 'give', 'go ahead',\n",
    "    'have you', 'have there', 'mail', 'is it', 'possible', \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503917/503917 [00:38<00:00, 13034.66it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def filter_sentences(sentences, intent_words):\n",
    "    if sentences is None:\n",
    "        return []  # Return an empty list for None values\n",
    "    return [sentence for sentence in sentences if any(keyword in sentence.lower() for keyword in intent_words)]\n",
    "\n",
    "df['targets'] = df['sentences'].progress_apply(lambda sentences: filter_sentences(sentences, intent_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Action-Object Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503917/503917 [2:02:40<00:00, 68.46it/s]   \n"
     ]
    }
   ],
   "source": [
    "def filter_action_object(targets):\n",
    "    pairs = []\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [{\"POS\": \"VERB\"}]\n",
    "    matcher.add(\"VERB\", [pattern])\n",
    "\n",
    "    for target in targets:\n",
    "        doc = nlp(target)\n",
    "        matches = matcher(doc)\n",
    "        match_list = []\n",
    "        for match_id, start, end in matches:\n",
    "            verb = doc[start]\n",
    "            for child in verb.children:\n",
    "                # Check for direct or prepositional object that is alphabetic\n",
    "                if child.dep_ in (\"dobj\", \"pobj\") and child.is_alpha:\n",
    "                    match_list.append(verb.lemma_ + \"_\" + child.text)\n",
    "        pairs.append(match_list)\n",
    "    return pairs\n",
    "\n",
    "df['action_object_pairs'] = df['targets'].progress_apply(lambda targets: filter_action_object(targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the columns action_object_pairs and targets to create individual row-entries for target-sentences,\n",
    "# assumingly containing intents with action-object-pairs extracted from the target-sentence\n",
    "df_exploded = df.explode(['action_object_pairs','targets'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the exploded DF\n",
    "df_exploded.to_parquet(\"../../data/processed/targets/avocado_train_targets_exploded.parquet\", engine=\"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_exp_intent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
