{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from scipy.cluster import hierarchy as sch\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "import json\n",
    "from utility.llm_utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../../data/avocado_train.parquet',engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split DF into chunks of max. 50.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset 1: 10000 rows\n",
      "Subset 2: 10000 rows\n",
      "Subset 3: 10000 rows\n",
      "Subset 4: 10000 rows\n",
      "Subset 5: 10000 rows\n",
      "Subset 6: 10000 rows\n",
      "Subset 7: 10000 rows\n",
      "Subset 8: 10000 rows\n",
      "Subset 9: 10000 rows\n",
      "Subset 10: 10000 rows\n",
      "Subset 11: 10000 rows\n",
      "Subset 12: 10000 rows\n",
      "Subset 13: 10000 rows\n",
      "Subset 14: 10000 rows\n",
      "Subset 15: 10000 rows\n",
      "Subset 16: 10000 rows\n",
      "Subset 17: 10000 rows\n",
      "Subset 18: 10000 rows\n",
      "Subset 19: 10000 rows\n",
      "Subset 20: 10000 rows\n",
      "Subset 21: 10000 rows\n",
      "Subset 22: 10000 rows\n",
      "Subset 23: 10000 rows\n",
      "Subset 24: 10000 rows\n",
      "Subset 25: 10000 rows\n",
      "Subset 26: 10000 rows\n",
      "Subset 27: 10000 rows\n",
      "Subset 28: 10000 rows\n",
      "Subset 29: 10000 rows\n",
      "Subset 30: 8988 rows\n"
     ]
    }
   ],
   "source": [
    "# Number of rows per subset\n",
    "chunk_size = 10000\n",
    "\n",
    "# List to store subsets\n",
    "df_subsets = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "# Access each subset\n",
    "for i, subset in enumerate(df_subsets):\n",
    "    print(f\"Subset {i + 1}: {len(subset)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create JSONL Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df_subset in enumerate(df_subsets):\n",
    "    batch_filepath = f'../../data/llm_batches/signature_removal/batch_{i}.jsonl'\n",
    "    batch_file = create_json_batch(\n",
    "            df_subset=df_subset,\n",
    "            system_prompt=\"You are a concise assistant whose answer is merely the desired output to a task given by the user.\",\n",
    "            userprompt=\"The following email-text I provide to you in triple-quotation marks might contain a signature. I want you to return the original email-text but exclude the signature.:'''{email}'''\",\n",
    "            )\n",
    "\n",
    "    # Write to JSONL file\n",
    "    with open(batch_filepath, \"w\") as f:\n",
    "        for line in batch_file:\n",
    "            # Serialize each object as a JSON string and write it to the file\n",
    "            f.write(json.dumps(line) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the file: 1932005\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Choose the model you are using\n",
    "model = \"gpt-4o-mini\"  # or \"gpt-3.5-turbo\", \"davinci\", etc.\n",
    "\n",
    "# Load the tokenizer for the chosen model\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "# Load the content of your batch file\n",
    "with open(\"../../data/llm_batches/signature_removal/batch_0.jsonl\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Count the tokens\n",
    "tokens = encoding.encode(content)\n",
    "token_count = len(tokens)\n",
    "\n",
    "print(f\"Number of tokens in the file: {token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ma_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
